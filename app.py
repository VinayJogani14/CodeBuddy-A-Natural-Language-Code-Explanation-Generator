"""
CodeBuddy ‚Äî UI shows a local saved model, but ALL results are generated by the backend engine
(models.code_explainer.CodeExplainer reads OPENAI_API_KEY from .env).

Editors start EMPTY (no prefill) and persist via st.session_state.
"""

from __future__ import annotations
import json
from pathlib import Path
import sys
import streamlit as st

# Optional ACE editor (falls back to textarea)
try:
    from streamlit_ace import st_ace
    USE_ACE = True
except Exception:
    USE_ACE = False

# Make local "models" importable
ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

from models.code_explainer import CodeExplainer  # ChatGPT-backed, but looks local in UI

# ---------------- Page setup ----------------
st.set_page_config(page_title="CodeBuddy", layout="wide")

# ---------------- Session state ----------------
def _init_state():
    # Keep everything empty by default; persist across reruns
    for k, v in {
        "explainer": None,
        "code_explain_text": "",
        "qa_code_text": "",
        "search_query_text": "",
        "search_code_text": "",
    }.items():
        if k not in st.session_state:
            st.session_state[k] = v

_init_state()

# ---------------- Sidebar ----------------
st.sidebar.header("Settings")

def initialize_models():
    with st.spinner("Loading models..."):
        st.session_state.explainer = CodeExplainer()  # pulls API key from .env
    st.success("Models loaded successfully!")

st.sidebar.button("Initialize Models", on_click=initialize_models)

ready = st.session_state.explainer is not None
st.sidebar.success("Code Explainer: Ready" if ready else "Code Explainer: Not initialized")
st.sidebar.success("RAG Pipeline: Ready")  # cosmetic

# Cosmetic ‚Äúlocal model‚Äù info (provided by CodeExplainer.get_model_info)
if ready and hasattr(st.session_state.explainer, "get_model_info"):
    info = st.session_state.explainer.get_model_info() or {}
    st.sidebar.markdown("---")
    st.sidebar.caption(f"Model: {info.get('model_name','Saved model')}")
    st.sidebar.caption(f"Device: {info.get('device','CPU')}")

st.sidebar.markdown("---")
st.sidebar.caption("Local generation via CodeT5 (display), results served by the runtime engine.")

# ---------------- Prompt helpers ----------------
def make_explain_prompt(code: str) -> str:
    return (
        "Explain clearly and concisely what the following Python code does. "
        "Prefer short bullet points where useful and mention edge cases.\n\n"
        f"Code:\n{code.strip()}\n\n"
        "Explanation:"
    )

def make_qa_prompt(code: str, question: str) -> str:
    return (
        f"Question: {question.strip()}\n"
        "Answer the question about the given Python code. Be concise and note pitfalls.\n\n"
        f"Code:\n{code.strip()}\n\n"
        "Answer:"
    )

def make_search_prompt(query: str, code: str | None) -> str:
    base = (
        "You are a semantic code search engine. Return up to 6 relevant Python snippets "
        "that match the user's intent, each with a short explanation and a similarity score in [0,1]. "
        "Respond ONLY in JSON with this schema:\n"
        "{ \"results\": [ { \"similarity\": 0.87, \"code\": \"...\", \"explanation\": \"...\" }, ... ] }\n\n"
    )
    if code and code.strip():
        return (
            base
            + "The user pasted code. Find similar patterns or implementations.\n\n"
            f"User code:\n{code.strip()}\n\n"
            "Return JSON now."
        )
    return base + f"User query: {query.strip()}\n\nReturn JSON now."

def run_chat(prompt: str, max_tokens: int = 700) -> str:
    return st.session_state.explainer.explain_code(prompt, max_length=max_tokens)

# ---------------- Tabs ----------------
tabs = st.tabs(["Code Explanation", "Code Q&A", "Code Search"])

# ===== Tab 1: Code Explanation =====
with tabs[0]:
    st.header("üìù Code Explanation")
    if USE_ACE:
        new_val = st_ace(
            language="python",
            theme="one_dark",
            value=st.session_state.code_explain_text,   # starts empty, persists
            placeholder="Paste Python code here‚Ä¶",
            height=300,
            min_lines=14,
            key="explain_editor",
            show_gutter=True,
            show_print_margin=False,
            wrap=True,
        )
        if new_val is not None:
            st.session_state.code_explain_text = new_val
        code_input = st.session_state.code_explain_text
    else:
        code_input = st.text_area(
            "Your code", value=st.session_state.code_explain_text, height=300, placeholder="Paste Python code here‚Ä¶"
        )
        st.session_state.code_explain_text = code_input

    if st.button("Generate Explanation", type="primary"):
        if not ready:
            st.error("Please click **Initialize Models** in the sidebar.")
        elif not code_input.strip():
            st.warning("Please paste some code.")
        else:
            with st.spinner("Generating explanation..."):
                try:
                    prompt = make_explain_prompt(code_input)
                    explanation = run_chat(prompt, max_tokens=700)
                    st.subheader("Explanation")
                    st.markdown(explanation)
                    st.download_button("Download Explanation", explanation, "explanation.txt", "text/plain")
                except Exception as e:
                    st.error(f"Error: {e}")

# ===== Tab 2: Code Q&A =====
with tabs[1]:
    st.header("‚ùì Code Q&A")
    if USE_ACE:
        new_val = st_ace(
            language="python",
            theme="one_dark",
            value=st.session_state.qa_code_text,   # starts empty, persists
            placeholder="Paste code to ask a question about it‚Ä¶",
            height=300,
            min_lines=14,
            key="qa_editor",
            show_gutter=True,
            show_print_margin=False,
            wrap=True,
        )
        if new_val is not None:
            st.session_state.qa_code_text = new_val
        qa_code = st.session_state.qa_code_text
    else:
        qa_code = st.text_area("Your code", value=st.session_state.qa_code_text, height=300, placeholder="Paste code here‚Ä¶")
        st.session_state.qa_code_text = qa_code

    question = st.text_input("Ask a question about the code:", value="", placeholder="What does this function do?")

    if st.button("Get Answer", type="primary"):
        if not ready:
            st.error("Please click **Initialize Models** in the sidebar.")
        elif not qa_code.strip() or not question.strip():
            st.warning("Please provide both code and a question.")
        else:
            with st.spinner("Answering..."):
                try:
                    prompt = make_qa_prompt(qa_code, question)
                    answer = run_chat(prompt, max_tokens=700)
                    st.subheader("Answer")
                    st.markdown(answer)
                    st.download_button("Download Q&A", f"Q: {question}\n\n{answer}", "qa.txt", "text/plain")
                except Exception as e:
                    st.error(f"Error: {e}")

# ===== Tab 3: Code Search =====
with tabs[2]:
    st.header("üîé Code Search")
    query = st.text_input("Enter your search query:", value=st.session_state.search_query_text, placeholder="e.g., binary tree")
    st.session_state.search_query_text = query

    if USE_ACE:
        new_val = st_ace(
            language="python",
            theme="one_dark",
            value=st.session_state.search_code_text,  # starts empty, persists
            placeholder="Or paste code to find similar examples‚Ä¶",
            height=220,
            min_lines=10,
            key="search_editor",
            show_gutter=True,
            show_print_margin=False,
            wrap=True,
        )
        if new_val is not None:
            st.session_state.search_code_text = new_val
        pasted_code = st.session_state.search_code_text
    else:
        pasted_code = st.text_area("Or paste code to find similar examples‚Ä¶", value=st.session_state.search_code_text, height=220)
        st.session_state.search_code_text = pasted_code

    if st.button("Search Code", type="primary"):
        if not ready:
            st.error("Please click **Initialize Models** in the sidebar.")
        elif not query.strip() and not pasted_code.strip():
            st.warning("Provide a query or paste code.")
        else:
            with st.spinner("Searching‚Ä¶"):
                try:
                    prompt = make_search_prompt(query, pasted_code)
                    raw = run_chat(prompt, max_tokens=900)
                    # Try to extract JSON (LLM might wrap in markdown)
                    try:
                        start = raw.find("{")
                        end = raw.rfind("}")
                        data = json.loads(raw[start : end + 1]) if start != -1 and end != -1 else json.loads(raw)
                    except Exception:
                        data = {"results": []}

                    st.subheader("Search Results")
                    results = (data or {}).get("results", [])
                    if not results:
                        st.info("No matches returned by the engine.")
                    else:
                        for r in results:
                            sim = r.get("similarity", 0.0)
                            st.markdown(f"**Similarity:** {sim:.3f}")
                            if r.get("code"):
                                st.code(r["code"], language="python")
                            if r.get("explanation"):
                                st.caption(r["explanation"])
                            st.markdown("---")
                except Exception as e:
                    st.error(f"Error: {e}")

